#!/usr/bin/env python
# coding: utf-8

# # WEB SCRAPING – ASSIGNMENT 4

# ### 1. Scrape the details of most viewed videos on YouTube from Wikipedia.
# Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos. You need to find following details:
# - A) Rank
# - B) Name
# - C) Artist
# - D) Upload date
# - E) Views

# In[1]:


# Importing Libraries
import selenium
import pandas as pd
import time
from bs4 import BeautifulSoup

# Importing selenium webdriver 
from selenium import webdriver
from selenium.webdriver.common.by import By

# Importing required Exceptions which needs to handled
from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException, ElementNotInteractableException

#Importing requests
import requests

# importing regex
import re

import warnings
warnings.filterwarnings('ignore')
import time


# In[9]:


# let's first connect to the web driver
driver = webdriver.Chrome(r"C:\Users\Sunny\Downloads\chromedriver_win32\chromedriver.exe")
time.sleep(3)


# Opening the homepage
url = "https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos"
driver.get(url)
time.sleep(2)


# In[10]:


#create empty lists
Rank = []
Name = []
Artist = []
UploadDate = []
Views = []


# In[11]:


# Scraping Rank of the videos
try:
    for i in driver.find_elements(By.XPATH,"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[1]"):
        Rank.append(i.text)
except NoSuchElementException:
    Rank.append("_")
    
    
#scrapping name
try:
    for i in driver.find_elements(By.XPATH,"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[2]"):
        Name.append(i.text)
except NoSuchElementException:
    Name.append("_")
    
    
#scrapping Artist
try:
    for i in driver.find_elements(By.XPATH,'//*[@id="mw-content-text"]/div[1]/table[2]/tbody/tr/td[3]'):
        Artist.append(i.text)
except NoSuchElementException:
    Artist.append("_")
    
    
#Scrapping Date
try:
    for i in driver.find_elements(By.XPATH,' //table[@class="wikitable sortable jquery-tablesorter"][1]/tbody/tr/td[5]'):
        UploadDate.append(i.text)
except NoSuchElementException:
    UploadDate.append('-')
# Scraping Views of videos


try:
    for i in driver.find_elements(By.XPATH,"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[4]"):
        Views.append(i.text)
except NoSuchElementException:
    Views.append("_") 


# In[12]:


print(len(Rank),len(Name),len(Artist),len(UploadDate),len(Views))


# In[13]:


# Creating DataFrame from scraped data
wikipedia = pd.DataFrame({})
wikipedia['Rank']=Rank
wikipedia['Name']=Name
wikipedia['Artist']=Artist
wikipedia['Upload Date']=UploadDate
wikipedia['Views']=Views


# In[14]:


wikipedia


# In[15]:


driver.close()


# ### 2. Scrape the details team India’s international fixtures from bcci.tv.
# Url = https://www.bcci.tv/. You need to find following details:
# - A) Match title (I.e. 1st ODI)
# - B) Series
# - C) Place
# - D) Date
# - E) Time
# Note: - From bcci.tv home page you have reach to the international fixture page through code.

# In[16]:


# let's first connect to the web driver
driver = webdriver.Chrome(r"C:\Users\Sunny\Downloads\chromedriver_win32\chromedriver.exe")
time.sleep(3)


# Opening the homepage
url = "https://www.bcci.tv/"
driver.get(url)
time.sleep(2)


# In[17]:


#clicking on International tab
International = driver.find_element(By.XPATH,'/html/body/nav/div[1]/div[2]/ul[1]/li[2]/a') # click button
International.click()


# In[18]:


#Creating Empty List
Name=[]
Series=[]
Place=[]
Date=[]
Time=[]

#Scrapping Name
try:
    for i in driver.find_elements(By.XPATH,'//span[@class="matchOrderText ng-binding ng-scope"]'):
        Name.append(i.text.replace('-',""))
except NoSuchElementException:
    Name.append('-')
    
#Scrapping Series
try:
    
    for i in driver.find_elements(By.XPATH,'//span[@class="ng-binding"]'):
        Series.append(i.text)
except NoSuchElementException:
    Series.append('-')
    
#scrapping Place
try:
    for i in driver.find_elements(By.XPATH,'//span[@class="ng-binding ng-scope"]'):
        Place.append(i.text)
except NoSuchElementException:
    Place.append('-')
    
#Scrapping Date
try:
    for i in driver.find_elements(By.XPATH,'//h5[@class="ng-binding"]'):
        Date.append(i.text)
except NoSuchElementException:
    Date.append('-')
    
#Scrapping Time
try:
    for i in driver.find_elements(By.XPATH,'//h5[@class="text-right ng-binding"]'):
        Time.append(i.text.replace('IST',""))
except NoSuchElementException:
    Time.append('-')


# In[19]:


# Create Dataframe

Fixtures=pd.DataFrame({})
Fixtures['Title']=Series
Fixtures['Name']=Name
Fixtures['Place']=Place
Fixtures['Date']=Date
Fixtures['Time']=Time
Fixtures


# In[20]:


driver.close()


# ### 3. Scrape the details of selenium exception from guru99.com.
# Url = https://www.guru99.com/: You need to find following details:
# - A) Name
# - B) Description
# 
# ##### Note: - From guru99 home page you have to reach to selenium exception handling page through code.

# In[25]:


# let's first connect to the web driver
driver = webdriver.Chrome(r"C:\Users\Sunny\Downloads\chromedriver_win32\chromedriver.exe")
time.sleep(3)


# Getting the webpage of mentioned url 
url = "https://www.guru99.com/"
driver.get(url)


# In[26]:


# Click Selenium Button
Selenium = driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[2]/div[1]/div/ul[1]/li[3]/a') # click button
try:
    Selenium.click()
except ElementNotInteractableException:#handling element not clickable exception
    driver.get(Selenium.get_attribute('href'))
time.sleep(3)


# In[29]:


# Click selenium exception handling  Button
exception_handling = driver.find_element(By.XPATH,"/html/body/div[1]/div/div/div/main/div/article/div/div/table[5]/tbody/tr[34]/td[2]")
try:
    exception_handling.click()
except ElementNotInteractableException:  #if the above code doesn't work/is not clickable then, the below code will handle it
    driver.get(exception_handling.get_attribute('href'))


# In[30]:


name=[]
desc=[]
#Scrapping Name
for i in driver.find_elements(By.XPATH,'//table[@class="table table-striped"]/tbody/tr/td[1]'):
    name.append(i.text)
for i in driver.find_elements(By.XPATH,'//table[@class="table table-striped"]/tbody/tr/td[2]'):
    desc.append(i.text)


# In[31]:


print(len(name),len(desc))


# In[32]:


Selenium_Exception=pd.DataFrame({})
Selenium_Exception['Exception name']=name
Selenium_Exception['Description']=desc
Selenium_Exception


# In[33]:


driver.close()


# ### 4. Scrape the details of State-wise GDP of India from statisticstime.com.
# Url = http://statisticstimes.com/: You have to find following details:
# - A) Rank
# - B) State
# - C) GSDP(18-19)
# - D) GSDP(17-18)
# - E) Share(2017)
# - F) GDP($ billion)
# ##### Note: - From statisticstimes home page you have to reach to economy page through code.

# In[34]:


# let's first connect to the web driver
driver = webdriver.Chrome(r"C:\Users\Sunny\Downloads\chromedriver_win32\chromedriver.exe")
time.sleep(3)


# Getting the webpage of mentioned url 
url = "http://statisticstimes.com/"
driver.get(url)


# In[35]:


driver.find_element(By.XPATH,'/html/body/div[1]/div/a').click()


# In[36]:


#Clicking on Economy button
driver.find_element(By.XPATH,"//div[@class='navbar']/div[2]/button").click()


# In[37]:


#Clicking on India
driver.find_element(By.XPATH,"//div[@class='dropdown-content']/a[3]").click()


# In[38]:


# Clicking on GDP of Indian Economy
GDP = driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a').click()
time.sleep(3)


# In[39]:


# Creating empty list
Rank = []
State = []
GSDP2 = []
Share = []
GDPbillion = []
# Scraping Rank
try:
    for i in driver.find_elements(By.XPATH,"//table[@class='display dataTable']/tbody/tr/td[1]"):
        Rank.append(i.text)
except NoSuchElementException:
    Rank.append("_")
# Scraping State
try:
    for i in driver.find_elements(By.XPATH,"//table[@class='display dataTable']/tbody/tr/td[2]"):
        State.append(i.text)
except NoSuchElementException:
    State.append("_")
# Scraping GSDP at current price (18-19)
try:
    for i in driver.find_elements(By.XPATH,"//table[@class='display dataTable']/tbody/tr/td[4]"):
        GSDP2.append(i.text)
except NoSuchElementException:
    GSDP2.append("_")
# Scraping Share (18-19)
try:
    for i in driver.find_elements(By.XPATH,"//table[@class='display dataTable']/tbody/tr/td[5]"):
        Share.append(i.text)
except NoSuchElementException:
    Share.append("_")
# Scraping GDP $ billion
try:
    for i in driver.find_elements(By.XPATH,"//table[@class='display dataTable']/tbody/tr/td[6]"):
        GDPbillion.append(i.text)
except NoSuchElementException:
    GDPbillion.append("_")


# In[40]:


INDIA=pd.DataFrame()
INDIA['RANK']=Rank
INDIA['STATE']=State
INDIA['GSDP2']=GSDP2
INDIA['SHARE']=Share
INDIA['GDPbillion']=GDPbillion
INDIA


# In[41]:


driver.close()


# ### 5. Scrape the details of trending repositories on Github.com.
# Url = https://github.com/ You have to find the following details:
# - A) Repository title
# - B) Repository description
# - C) Contributors count
# - D) Language used
# ##### Note: - From the home page you have to click on the trending option from Explore menu through code.

# In[49]:


# let's first connect to the web driver
driver = webdriver.Chrome(r"C:\Users\Sunny\Downloads\chromedriver_win32\chromedriver.exe")
time.sleep(3)


# Getting the webpage of mentioned url 
url = " https://github.com/"
driver.get(url)


# In[51]:


# Click on explore

explore = driver.find_element(By.XPATH,"/html/body/div[1]/header/div[3]/nav/a[5]")
try:
    explore.click()
    time.sleep(5)
except ElementNotInteractableException:
    driver.get(explore.get_attribute('href'))


# In[52]:


# Click on Trending

trending = driver.find_element(By.XPATH,"/html/body/div[5]/main/div[1]/nav/div/a[3]")
try:
    driver.get(trending.get_attribute('href'))
except ElementNotInteractableException:
    driver.get(trending.get_attribute('href'))


# In[53]:


#Creating empty list:
URLs = []
repository_title = []
Description = []
Contributors = []
Language = []
lang = []

#Fetching urls for each repository
repository = driver.find_elements(By.XPATH,"//h1[@class = 'h3 lh-condensed']//a")
for i in repository:
    URLs.append(i.get_attribute("href"))


# In[54]:


# Scraping Repository Title data
title = driver.find_elements(By.XPATH,"//h1[@class = 'h3 lh-condensed']")
for i in title:
    repository_title.append(i.text)


# In[55]:


# Scraping data from all repository page
for i in URLs:
    driver.get(i)
    time.sleep(2)
    
# Scraping Repository Description data
    try:
        desc = driver.find_element(By.XPATH,"//p[@class='f4 my-3']")
        Description.append(desc.text)
    except NoSuchElementException:
        Description.append('-')
        
# Scraping Contributors Count data
    try:
        contributor = driver.find_element(By.XPATH,"//*[contains(text(),'    Contributors ')]")
        Contributors.append(contributor.text.replace('Contributors',''))
    except NoSuchElementException:
        Contributors.append('-')
        
        
# Scraping Languages used data
    lang=[]
    try:
        for i in driver.find_elements(By.XPATH,'//span[@class="color-fg-default text-bold mr-1"]'):
            lang.append(i.text)
        Language.append(lang)
    except NoSuchElementException:
        lang.append('-')


# In[56]:


print(len(repository_title),len(Description),len(Contributors),len(Language))


# In[57]:


#Data Frame

Github=pd.DataFrame({})
Github['Repository title'] = repository_title
Github['Repository description'] = Description
Github['Contributors count'] = Contributors
Github['Language used'] = Language
Github


# In[58]:


driver.close()


# ### 6. Scrape the details of top 100 songs on billiboard.com.
# Url = https:/www.billboard.com/ You have to find the following details:
# - A) Song name
# - B) Artist name
# - C) Last week rank
# - D) Peak rank
# - E) Weeks on board
# ##### Note: - From the home page you have to click on the charts option then hot 100-page link through code.

# In[63]:


# let's first connect to the web driver
driver = webdriver.Chrome(r"C:\Users\Sunny\Downloads\chromedriver_win32\chromedriver.exe")
time.sleep(3)


# Getting the webpage of mentioned url 
url = "https:/www.billboard.com/"
driver.get(url)


# In[64]:


#Clicking on charts
charts=driver.find_element(By.XPATH,'//*[@id="main-wrapper"]/header/div[2]/div/nav/ul/li[1]/a').click()


# In[65]:


# Creating empty lists
Song_Name = []
Artist_Name = []
Last_week_rank = []
Peak_rank = []
Weeks_on_board = []


# In[66]:


# Scraping name
for i in driver.find_elements(By.XPATH,'//li[@class="lrv-u-width-100p"]'):
    Song_Name.append(i.text)
len(Song_Name)


# In[67]:


#Scrappin Artist name 1 st one
Artist_Name.append(driver.find_element(By.XPATH,"//span[@class='c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only u-font-size-20@tablet']").text)

#Remainig Artist Name
artistTag=driver.find_elements(By.XPATH,"//span[@class='c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only']")
Artist_Name.extend([i.text for i in artistTag])

#Scapping Rank
rank=[]
rankTag=driver.find_elements(By.XPATH,"//span[@class='c-label  a-font-primary-bold-l a-font-primary-m@mobile-max u-font-weight-normal@mobile-max lrv-u-padding-tb-050@mobile-max u-font-size-32@tablet']")
rank.extend([i.text for i in rankTag[:3]])

#Remaining RAnk
Rank=[]
RankTag=driver.find_elements(By.XPATH,"//span[@class='c-label  a-font-primary-m lrv-u-padding-tb-050@mobile-max']")
Rank.extend([i.text for i in RankTag])


# In[68]:


# removing ''
for i in Rank:
    if i=='':
        Rank.remove(i)


# In[69]:


# slicing as per requirement
lastweekpos=Rank[0::3]
lastweekpos.insert(0,rank[0])
peakPos=Rank[1::3]
peakPos.insert(0,rank[1])
weeksonBoard=Rank[2::3]
weeksonBoard.insert(0,rank[2])


# In[70]:


#check length of all
len(Song_Name),len(Artist_Name),len(lastweekpos),len(peakPos),len(weeksonBoard)


# In[71]:


# Create Dataframe
df=pd.DataFrame()
df['SongName']=Song_Name[0:100]
df['ArtistName']=Artist_Name
df['Last Week']=lastweekpos[0:100]
df["PeekPosition"]=peakPos[0:100]
df['Weeks On board']=weeksonBoard[0:100]
df


# In[72]:


driver.close()


# ### 7. Scrape the details of Data science recruiters from naukri.com.
# Url = https://www.naukri.com/ You have to find the following details:
# - A) Name
# - B) Designation
# - C) Company
# - D) Skills they hire for
# - E) Location
# ##### Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and click on search. All this should be done through code

# In[87]:


# let's first connect to the web driver
driver = webdriver.Chrome(r"C:\Users\Sunny\Downloads\chromedriver_win32\chromedriver.exe")
time.sleep(3)


# Getting the webpage of mentioned url 
url = "https://www.naukri.com/"
driver.get(url)


# In[88]:


#Fetching urls to navigate recruiter page
recruiter = driver.find_element(By.XPATH,'//a[@title="Search Jobs"]')
page_url = recruiter.get_attribute("href")

driver.get(page_url)
time.sleep(3)


# In[89]:


# Fetching search button, sending keys and clicking on it
search = driver.find_element(By.XPATH,"/html/body/div[1]/div[6]/div/div/div[1]/div/div/div/input") 
search.send_keys("Data science ")           
btn = driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[6]').click()     
time.sleep(3)


# In[90]:


#Creating empty lists
Name = []
Designation = []
Company = []
Skills = []
Location = []


# In[95]:


#Scraping data of Names
for i in driver.find_elements(By.XPATH,"//div[@class='job-description fs12 grey-text']"):
    Name.append(i.text)
time.sleep(3)

#Scraping data of Designation
for i in driver.find_elements(By.XPATH,"//a[@class='title fw500 ellipsis']"):
    Designation.append(i.text)
    
#Scraping data of company name
for i in driver.find_elements(By.XPATH,"//a[@class='subTitle ellipsis fleft']"):
    Company.append(i.text)

#Scraping data of locations
for i in driver.find_elements(By.XPATH,"//span[@class='ellipsis fleft']"):
    Location.append(i.text)
time.sleep(3)

#Scraping data of skills
for i in driver.find_elements(By.XPATH,"//ul[@class='tags has-description']"):
    Skills.append(i.text)


# In[96]:


print(len(Name),len(Designation),len(Company),len(Location),len(Skills))


# In[97]:


# Creating dataframe for scraped data
Naukri=pd.DataFrame({})
Naukri['Name'] = Name[0:34]
Naukri['Designation'] = Designation[0:34]
Naukri['Company'] = Company[0:34]
Naukri['Location'] = Location[0:34]
Naukri['Skills']=Skills[0:34]
Naukri


# In[98]:


driver.close()


# ### 8. Scrape the details of Highest selling novels.
# Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-greycompare/
# You have to find the following details:
# - A) Book name
# - B) Author name
# - C) Volumes sold
# - D) Publisher
# - E) Genre

# In[2]:


# let's first connect to the web driver
driver = webdriver.Chrome(r"C:\Users\Sunny\Downloads\chromedriver_win32\chromedriver.exe")
time.sleep(3)


# Getting the webpage of mentioned url 
url = "https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
driver.get(url)


# In[3]:


#Creating empty list
Bookname = []
Authorname = []
Volumessold = []
Publisher = []
Genre = []


# In[4]:


#scraping book names data
for i in driver.find_elements(By.XPATH,"//tbody//tr/td[2]"):
    Bookname.append(i.text)
    
#Scraping author names data
for i in driver.find_elements(By.XPATH,"//tbody/tr/td[3]"):
    try:
        if i.text == '0' : raise NoSuchElementException
        Authorname.append(i.text)
    except NoSuchElementException:
        Authorname.append('-')
time.sleep(1)

#Scraping data of volumes sold
for i in driver.find_elements(By.XPATH,"//tbody/tr/td[4]"):
    Volumessold.append(i.text)
    
#Scraping data of publisher names
for i in driver.find_elements(By.XPATH,"//tbody/tr/td[5]"):
    Publisher.append(i.text)
    
#Scraping data of genre
for i in driver.find_elements(By.XPATH,"//tbody/tr/td[6]"):
    Genre.append(i.text)


# In[5]:


print(len(Bookname),len(Authorname),len(Volumessold),len(Publisher),len(Genre))


# In[6]:


#Creating dataframe for scraped data
Novels=pd.DataFrame({})
Novels['Book Name'] = Bookname
Novels['Author'] = Authorname
Novels['Volume sold'] = Volumessold
Novels['Publisher'] = Publisher
Novels['Genre'] = Genre
Novels


# In[7]:


driver.close()


# ### 9. Scrape the details most watched tv series of all time from imdb.com.
# Url = https://www.imdb.com/list/ls095964455/
# You have to find the following details:
# - A) Name
# - B) Year span
# - C) Genre
# - D) Run time
# - E) Ratings
# - F) Votes

# In[8]:


# let's first connect to the web driver
driver = webdriver.Chrome(r"C:\Users\Sunny\Downloads\chromedriver_win32\chromedriver.exe")
time.sleep(3)


# Getting the webpage of mentioned url 
url = " https://www.imdb.com/list/ls095964455/ "
driver.get(url)


# In[9]:


#Creating empty lists.
Name = []
Year_span = []
Genre = []
Run_time = []
Ratings = []
Votes = []


# In[10]:


#Scraping data of Names
for i in driver.find_elements(By.XPATH,"//h3[@class='lister-item-header']/a"):
    Name.append(i.text)
    
#Scraping data of Year span
for i in driver.find_elements(By.XPATH,"//span[@class='lister-item-year text-muted unbold']"):
    Year_span.append(i.text)
    
#Scraping data of Run time
for i in driver.find_elements(By.XPATH,"//span[@class='runtime']"):
    Run_time.append(i.text)
    
#Scraping data of Ratings
for i in driver.find_elements(By.XPATH,"//div[@class='ipl-rating-star small']//span[2]"):
    Ratings.append(i.text)
    
#Scraping data of genre
for i in driver.find_elements(By.XPATH,"//span[@class='genre']"):
    Genre.append(i.text)
    
#Scraping data of votes
for i in driver.find_elements(By.XPATH,"//div[@class='lister-item-content']//p[4]/span[2]"):
    Votes.append(i.text)


# In[11]:


print(len(Name),len(Year_span),len(Run_time),len(Ratings),len(Genre),len(Votes))


# In[12]:


series=pd.DataFrame({})
series['Name'] = Name
series['Year Span'] = Year_span
series['Genre'] = Genre
series['Run Time'] = Run_time
series['Ratings'] = Ratings
series['Votes'] = Votes
series


# In[13]:


driver.close()


# ### 10. Details of Datasets from UCI machine learning repositories.
# Url = https://archive.ics.uci.edu/
# You have to find the following details:
# - A) Dataset name
# - B) Data type
# - C) Task
# - D) Attribute type
# - E) No of instances
# - F) No of attribute
# - G) Year
# ##### Note: - from the home page you have to go to the ShowAllDataset page through code

# In[14]:


# let's first connect to the web driver
driver = webdriver.Chrome(r"C:\Users\Sunny\Downloads\chromedriver_win32\chromedriver.exe")
time.sleep(3)


# Getting the webpage of mentioned url 
url = "https://archive.ics.uci.edu/"
driver.get(url)


# In[15]:


# Finding view all dataset button from the webpage
viewall_dataset = driver.find_element(By.XPATH,"//tbody[1]//tr/td[2]/span[2]/a")    
page_url = viewall_dataset.get_attribute("href")
driver.get(page_url)


# In[16]:


# Fetching page urls of all datasets 
view_list = driver.find_element(By.XPATH,"/html/body/table[2]/tbody/tr/td[2]/table[1]/tbody/tr/td[2]/p/a")  
list_url = view_list.get_attribute("href")           
driver.get(list_url)


# In[17]:


# Fetching urls for each dataset
dataset_url = driver.find_elements(By.XPATH,"//p[@class='normal']//b/a")    

urls = []     
for i in dataset_url:
    urls.append(i.get_attribute("href"))


# In[20]:


#Creating empty lists
Dataset_name = []
Data_type = []
Task = []
Attribute_type = []
No_of_instances = []
No_of_attributes = []
Year = []


# In[21]:


for i in urls:
    driver.get(i)
    time.sleep(2)
    
    #Scraping Dataset name
    try: 
        dataset_name = driver.find_element(By.XPATH,"//span[@class='heading']")
        Dataset_name.append(dataset_name.text)
    except NoSuchElementException:
        Dataset_name.append('-')
    
    
    #Scraping data type
    try:
        data_type = driver.find_element(By.XPATH,"//table[@border='1']//tbody/tr/td[2]")
        if data_type.text == "N/A": raise NoSuchElementException
        Data_type.append(data_type.text)
    except NoSuchElementException:
        Data_type.append('-')
    
    
    #scraping Task
    try:
        task = driver.find_element(By.XPATH,"//table[@border='1']//tbody/tr[3]/td[2]")
        if task.text == "N/A": raise NoSuchElementException
        Task.append(task.text)
    except NoSuchElementException:
        Task.append('-')
    
    
    # Scraping Attribute type
    try:
        attribute_type = driver.find_element(By.XPATH,"//table[@border='1']//tbody/tr[2]/td[2]")
        if attribute_type.text == "N/A": raise NoSuchElementException
        Attribute_type.append(attribute_type.text)
    except NoSuchElementException:
        Attribute_type.append('-')
    
    
    # Scraping No of instances
    try:
        instances = driver.find_element(By.XPATH,"//table[@border='1']//tbody/tr/td[4]")
        if instances.text == "N/A": raise NoSuchElementException
        No_of_instances.append(instances.text)
    except NoSuchElementException:
        No_of_instances.append('-')
    
    
    # Scraping No of attributes
    try:
        attribute = driver.find_element(By.XPATH,"//table[@border='1']//tbody/tr[2]/td[4]")
        if attribute.text == "N/A": raise NoSuchElementException
        No_of_attributes.append(attribute.text)
    except NoSuchElementException:
        No_of_attributes.append('-')
    
    
    # Scraping year
    try:
        year = driver.find_element(By.XPATH,"//table[@border='1']//tbody/tr[2]/td[6]")
        if year.text == "N/A": raise NoSuchElementException
        Year.append(year.text[:4])
    except NoSuchElementException:
        Year.append('-')


# In[22]:


print(len(Dataset_name),len(Data_type),len(Task),len(Attribute_type),len(No_of_instances),len(No_of_attributes),len(Year))


# In[23]:


# Creating dataframe 

MachineLearning=pd.DataFrame({})
MachineLearning['Data Name'] = Dataset_name[:622]
MachineLearning['Data Type'] = Data_type[:622]
MachineLearning['Task'] = Task[:622]
MachineLearning['Attribute type'] = Attribute_type[:622]
MachineLearning['No of instance'] = No_of_instances[:622]
MachineLearning['No of attributes'] = No_of_attributes[:622]
MachineLearning['Year'] = Year[:622]
MachineLearning


# In[24]:


driver.close()

